{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Analysis of Food Recipes and Reviews\n",
    "\n",
    "**Name(s)**: Kristen Lee, Jordi Pham\n",
    "\n",
    "**Website Link**: <a href='https://kristen-lee-120.github.io/wokingwithdata/'> Woking with Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.652554Z",
     "start_time": "2019-10-31T23:36:27.180520Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "# from dsc80_utils import * # Feel free to uncomment and use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recipes and ratings dataset is a dataset that revolves around recipes for food and reviews for how well those recipes did. For our project, we are particularly interested in what is the relationship between cooking time and the average rating of recipes. Readers of should care about our dataset and questions because they can provide a baseline when choosing a recipe to cook for their next meal. Per our given csv files, `interactions` is a dataset with 731927 rows, `recipes` is a dataset with 83782 rows, and the left-merged dataset evaluates to 234429 rows of data. The columns that will prove most relevant to our question are `rating` (user-given rating of the recipe) and `minutes` (preparation time of the recipe). Using these two columns, we believe we will be able to compile the right information to hopefully answer our data science question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('../RAW_recipes_copy.csv')\n",
    "interactions = pd.read_csv('../RAW_interactions_copy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(recipes.shape)\n",
    "recipes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(interactions.shape)\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left Merge Interactions to Recipes - Keep All Recipes\n",
    "main = recipes.merge(interactions, how='left', left_on='id', right_on='recipe_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(main.shape)\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Rating Column - 0 to NaN\n",
    "main['rating'] = main['rating'].replace(0, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view to check\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating an Average Rating Column\n",
    "# Creating Series of avg ratings\n",
    "avg_ratings = main.groupby('id')['rating'].mean()\n",
    "\n",
    "# map it onto the original df for the new column\n",
    "main['avg_rating'] = main['id'].map(avg_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view to check\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning Nutrition Column\n",
    "# Define column names\n",
    "nutrition_cols = ['calories', \n",
    "                  'total_fat_pdv', \n",
    "                  'sugar_pdv', \n",
    "                  'sodium_pdv', \n",
    "                  'protein_pdv', \n",
    "                  'saturated_fat_pdv', \n",
    "                  'carbohydrates_pdv']\n",
    "\n",
    "# Function to process nutrition data\n",
    "def process_nutrients(nutri_str):\n",
    "    try:\n",
    "        nutri_list = [float(x) for x in nutri_str.strip('[]').split(', ')]  # Convert to float for numerical operations\n",
    "        if len(nutri_list) == len(nutrition_cols):\n",
    "            return nutri_list\n",
    "        else:\n",
    "            return [None] * len(nutrition_cols)  # Handle unexpected lengths\n",
    "    except:\n",
    "        return [None] * len(nutrition_cols)  # Handle errors gracefully\n",
    "\n",
    "# Apply function and expand into multiple columns\n",
    "main[nutrition_cols] = main['nutrition'].apply(process_nutrients).apply(pd.Series)\n",
    "\n",
    "# Drop the original column if no longer needed\n",
    "main.drop(columns=['nutrition'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view to check\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning tags and adding column for the number of tags per recipe\n",
    "main['tags'] = main['tags'].str.strip('[]').str.replace(\"'\", '').str.split(', ')\n",
    "main['n_tags'] = main['tags'].apply(lambda x: len(x))\n",
    "\n",
    "# view to check\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA's\n",
    "n_missing = main.isna().sum()\n",
    "n_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate, Bivariate, and Aggregate analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isolate some information\n",
    "time_n_rating = main[['id', 'minutes', 'n_steps', 'rating', 'avg_rating']].drop_duplicates()\n",
    "time_n_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Univariate\n",
    "# histogram post getting rid of the stupid ass million minute recipe\n",
    "Q1 = time_n_rating['minutes'].quantile(0.25)\n",
    "Q3 = time_n_rating['minutes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "outliers_condition = (time_n_rating['minutes'] > (Q3 + 1.5 * IQR)) | (time_n_rating['minutes'] < (Q1 - 1.5 * IQR))\n",
    "\n",
    "# Filter out the outliers\n",
    "clean_time = time_n_rating[~outliers_condition]\n",
    "\n",
    "px.histogram(clean_time, x = 'minutes', nbins=20, histnorm='density', title='Density Histogram of cook time for a recipe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a scatter plot with no outliers\n",
    "fig = px.scatter(clean_time, x='minutes', y=clean_time.index, title=\"Minutes for a recipe without Outliers\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of average rating\n",
    "px.histogram(time_n_rating[['avg_rating']], title='Average rating of different recipes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# box plot of average rating across recipes\n",
    "px.box(time_n_rating['avg_rating'], x='avg_rating', title = 'average rating of recipes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bivariate\n",
    "# without removing the outlier minutes scatterplot\n",
    "px.scatter(time_n_rating, x='avg_rating', y='minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the outlier minutes scatterplot with a line of best fit\n",
    "\n",
    "fig = px.scatter(clean_time, x='avg_rating', y='minutes', trendline='ols', title='Cook time vs Average Rating of recipes')\n",
    "fig.update_traces(line_color='red')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main[(main['minutes']==1680) & (main['avg_rating'] == 5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interesting aggregates\n",
    "main.pivot_table(index='avg_rating',\n",
    "               columns='n_steps',\n",
    "               values='minutes',\n",
    "               aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to read part of this graph is, say, let's look at the first cell where the average rating is 1 and the recipe takes 1 step. A recipe that has an average rating of 1 and takes 1 step has an average cooking time of 12 minutes flat. All other cells can be read in a similar manner.\n",
    "\n",
    "From this table, we can also see that the most amount of steps in the entire main dataframe is a 100-step recipe. At 100 steps and an average rating of 5, the average cook time was 1680 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Assessment of Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMAR Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# main[main.isna().any(axis=1)].head() -- do we need this cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reviews column is a column that could be deemed as NMAR, or not missing at random. After all, this is a dataset of food recipes and their respective reviews; a missing review for a recipe can really only be traced back to how a reviewer interacted with the recipe. The food could've have been so bad that the person felt no need to even leave a review, the recipe may or may not have ever been tried before for a review to be made, or human nature got in the way and the reviewer essentially just forgot to leave a review when logging their personal interaction. \n",
    "\n",
    "If we were to change the missingness from NMAR to MAR, where some other column in the dataset could explain the missingness of the `review` column, the additional data that we could possibly obtain would be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at cols with missing values again\n",
    "n_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.groupby('id')['review'].count() --- tbh dont need"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missingess: avg ratings on diff-of-means for protein pdv (percent daily value)\n",
    "\n",
    "Non-missingness: avg ratings on diff-of-means for calories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR elimination to deal with extreme outliers (maybe move this to cleaning)\n",
    "Q1 = main['minutes'].quantile(0.25)\n",
    "Q3 = main['minutes'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "iqr_conditions = ((main['minutes'] > (Q3 + IQR * 1.5)) | (main['minutes'] < (Q1 - IQR * 1.5)))\n",
    "iqr_main = main[~iqr_conditions]\n",
    "iqr_main['minutes'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Mechanism 1 - MAR between average rating and protein_pdv\n",
    "m_rating_n_tag = iqr_main[iqr_main['avg_rating'].isna()]['protein_pdv'].mean()\n",
    "nm_rating_n_tag = iqr_main[iqr_main['avg_rating'].notna()]['protein_pdv'].mean()\n",
    "m_t_stat1 = m_rating_n_tag - nm_rating_n_tag\n",
    "m_t_stat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle avg rating column\n",
    "n = 500\n",
    "shuffler1 = iqr_main.copy()\n",
    "\n",
    "diff_means1 = []\n",
    "for _ in range(n):\n",
    "    # shuffle the column \n",
    "    shuffler1['avg_rating'] = np.random.permutation(shuffler1['avg_rating'])\n",
    "    # determine the differences\n",
    "    dummy_na = shuffler1[shuffler1['avg_rating'].isna()]['protein_pdv'].mean()\n",
    "    dummy_isna = shuffler1[shuffler1['avg_rating'].notna()]['protein_pdv'].mean()\n",
    "    diff_means1.append(dummy_na - dummy_isna)\n",
    "    \n",
    "np.mean(np.array(diff_means1) >= np.abs(m_t_stat1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The p-value is 0.016, which is greater than our chosen significance level of 0.01. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Mechanisms 2 - MAR is nonexistent between average rating and calories\n",
    "m_rating_cal = main[main['avg_rating'].isna()]['calories'].mean()\n",
    "nm_rating_cal = main[main['avg_rating'].notna()]['calories'].mean()\n",
    "m_t_stat2 = m_rating_cal - nm_rating_cal\n",
    "m_t_stat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle avg rating column\n",
    "n = 500\n",
    "shuffler2 = main.copy()\n",
    "\n",
    "diff_means2 = []\n",
    "for _ in range(n):\n",
    "    # shuffle the column\n",
    "    shuffler2['avg_rating'] = np.random.permutation(shuffler2['avg_rating'])\n",
    "    # determine the differences\n",
    "    dummy_na = shuffler2[shuffler2['avg_rating'].isna()]['calories'].mean()\n",
    "    dummy_isna = shuffler2[shuffler2['avg_rating'].notna()]['calories'].mean()\n",
    "    diff_means2.append(np.abs(dummy_na - dummy_isna))\n",
    "\n",
    "np.mean(np.array(diff_means2) >= np.abs(m_t_stat2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null: There is no relationship between average rating of a recipe and its cooking time.\n",
    "\n",
    "Alt: There is a relationship between the average rating of a recipe and its cooking time \n",
    "\n",
    "Using Pearson's R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# observed test statistic, correlation between minutes and average rating\n",
    "obs_test_stat = iqr_main['minutes'].corr(iqr_main['avg_rating'])\n",
    "obs_test_stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "shuffler3 = iqr_main.copy()\n",
    "\n",
    "corrs = []\n",
    "for _ in range(n):\n",
    "    # shuffle the column \n",
    "    shuffler3['avg_rating'] = np.random.permutation(shuffler1['avg_rating'])\n",
    "    # determine the differences\n",
    "    dummy_corr = iqr_main['minutes'].corr(shuffler3['avg_rating'])\n",
    "    corrs.append(dummy_corr)\n",
    "\n",
    "np.mean(np.abs(corrs) >= np.abs(obs_test_stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the histogram using Plotly Express\n",
    "fig = px.histogram(\n",
    "    x=corrs, \n",
    "    nbins=30, \n",
    "    opacity=0.7, \n",
    "    title=\"Permutation Test: Distribution of Shuffled Correlations\",\n",
    "    labels={\"x\": \"Shuffled Correlations\", \"y\": \"Frequency\"},\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Add observed test statistic as a vertical line\n",
    "fig.add_vline(x=obs_test_stat, line=dict(color=\"red\", width=2, dash=\"dash\"), annotation_text=\"Observed Test Stat\", annotation_position=\"top\")\n",
    "fig.add_vline(x=-np.abs(obs_test_stat), line=dict(color=\"blue\", width=2, dash=\"dash\"), annotation_text=\"- |Observed|\", annotation_position=\"top left\")\n",
    "fig.add_vline(x=np.abs(obs_test_stat), line=dict(color=\"blue\", width=2, dash=\"dash\"), annotation_text=\"+ |Observed|\", annotation_position=\"top right\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shown with a scatterplot as well\n",
    "fig = px.scatter(iqr_main, x=\"minutes\", y=\"avg_rating\", \n",
    "                 title=\"Scatter Plot of Minutes vs. Avg Rating\",\n",
    "                 labels={\"minutes\": \"Minutes\", \"avg_rating\": \"Average Rating\"},\n",
    "                 template=\"plotly_white\",\n",
    "                 trendline=\"ols\")  # Ordinary Least Squares Regression\n",
    "\n",
    "# Compute Pearson correlation coefficient (r)\n",
    "pearson_r = iqr_main['minutes'].corr(iqr_main['avg_rating'])\n",
    "\n",
    "# Add annotation with Pearson's r value\n",
    "fig.add_annotation(\n",
    "    x=min(iqr_main[\"minutes\"]),  # Position on the x-axis\n",
    "    y=max(iqr_main[\"avg_rating\"]),  # Position on the y-axis\n",
    "    text=f\"Pearson's r = {pearson_r:.3f}\",  # Show r rounded to 3 decimals\n",
    "    showarrow=False,\n",
    "    font=dict(size=14, color=\"black\"),\n",
    "    align=\"left\",\n",
    "    xanchor=\"left\",\n",
    "    yanchor=\"top\"\n",
    ")\n",
    "\n",
    "# Show plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Framing a Prediction Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.657068Z",
     "start_time": "2019-10-31T23:36:28.654650Z"
    }
   },
   "source": [
    "We are planning to predict the number of steps of a recipe based on cooking time and number of ingredients. This type of prediction problem is a regression type problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# Predictive Models\n",
    "# Training predictive model to predict number of steps based on cooking time and number of ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('model', LinearRegression())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iqr_main[['minutes', 'n_ingredients']]\n",
    "Y = iqr_main['n_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.score(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Plotly\n",
    "results_df = pd.DataFrame({\n",
    "    'Actual Steps': Y,\n",
    "    'Predicted Steps': pipeline.predict(X)\n",
    "})\n",
    "\n",
    "# Scatter plot using Plotly\n",
    "fig = px.scatter(\n",
    "    results_df, \n",
    "    x='Actual Steps', \n",
    "    y='Predicted Steps',\n",
    "    title='Actual vs. Predicted Number of Steps',\n",
    "    opacity=0.5,  # Equivalent to `alpha` in matplotlib\n",
    ")\n",
    "\n",
    "# Final touches\n",
    "fig.update_layout(\n",
    "    xaxis_title='Actual Steps',\n",
    "    yaxis_title='Predicted Steps',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for Plotly\n",
    "results_df = pd.DataFrame({\n",
    "    'Predicted Steps': pipeline.predict(X)\n",
    "})\n",
    "\n",
    "# Distribution plot\n",
    "fig = px.histogram(\n",
    "    results_df, \n",
    "    x='Predicted Steps',\n",
    "    nbins=30,  # Adjust the number of bins as needed\n",
    "    title='Distribution of Predicted Steps',\n",
    "    marginal='box',  # Adds a box plot for additional insights\n",
    ")\n",
    "\n",
    "# Final touches\n",
    "fig.update_layout(\n",
    "    xaxis_title='Predicted Steps',\n",
    "    yaxis_title='Count',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot 1: Number of Steps vs Cooking Minutes\n",
    "fig1 = px.scatter(\n",
    "    iqr_main, \n",
    "    x='minutes', \n",
    "    y='n_steps',\n",
    "    title='Number of Steps vs Cooking Minutes',\n",
    "    labels={'minutes': 'Cooking Minutes', 'n_steps': 'Number of Steps'},\n",
    "    opacity=0.5\n",
    ")\n",
    "fig1.show()\n",
    "\n",
    "# Scatter plot 2: Number of Steps vs Number of Ingredients\n",
    "fig2 = px.scatter(\n",
    "    iqr_main, \n",
    "    x='n_ingredients', \n",
    "    y='n_steps',\n",
    "    title='Number of Steps vs Number of Ingredients',\n",
    "    labels={'n_ingredients': 'Number of Ingredients', 'n_steps': 'Number of Steps'},\n",
    "    opacity=0.5\n",
    ")\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.662099Z",
     "start_time": "2019-10-31T23:36:28.660016Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Fairness Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T23:36:28.666489Z",
     "start_time": "2019-10-31T23:36:28.664381Z"
    }
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
